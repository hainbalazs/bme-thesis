# Thesis: Modeling Explanations of Artificial Intelligence Solutions

This repository contains my research work leading up to my Bachelor's thesis at Budapest University of Technology and Economics.
This concludes a survey on current AI Explainability approaches which is followed up with a research project where I applied AIX tools Qualitative Modelling and the development of an explainable classifier model.

## Structure of the repository
- thesis.pdf: BSc Thesis on Modeling Explanations of Artificial Intelligence Solutions (written in English, only the first two pages are Hungarian)
- short-paper.pdf: A short paper format of Chapter 6, which I presented at the AIME Conference at Budapest in 2021.
- notebooks/: Jupiter notebooks containing relevant codesegments and my running example of the Thesis

## Abstract
The rapidly spreading solutions based on embedded artificial intelligence in cyber-physical systems have defined the behavioural model of complex systems with machine learning tools. In addition, a high-level overview of observed system is ideal - according to the concept of hybrid modelling - if a discrete model provides it. Thus, qualitative modelling has a prominent role in model-based supervisory control, as this paradigm implies the possibility of using the full repertoire of well-proven formal methods for the validation of discrete systems. However the application of qualitative models in CPS control raises specific challenges. Firstly, insufficient coverage of potentially dangerous operational domains may lead to hazardous control errors. Moreover, outliers need special care in critical applications.
Especially, the quality of the model - as the decisive factor of its faithfulness - must be guaranteed. Hybrid model mixtures supported by machine learning provided inferences require particular attention. A fundamental obstacle to their prevalence is that accurate modelling can only be satisfied with complex models that are opaque to the human observer. For this reason the interpretability of the model and the explanation of its behaviour are problematic, especially in mission-critical systems. This thesis focuses on the suitability of universal model explanatory tools and methods for qualitative abstractions of embedded AI models for V&V purposes.
In addition to the domain fields closely related to the topic, a general overview will be given on the major motivations, intentions, and challenges behind the concept of explainable artificial intelligence. These ideas justify the growing demand to improve the explainability of data sets and derived models (XAI). From the various available XAI libraries, different model explanatory approaches will be showcased with the use of two highlighted state-of-the-art explainability libraries (IBM AIX360, DALEX), demonstrating their effectiveness in a wide range of machine learning models with different interpretability characteristics.
In a designated chapter, the reader can also gain insight into the process of designing a unique directly explainable classification algorithm.
Ultimately, the research proposes an approach that allows for qualitative abstractionlevel validation of embedded models known even at the black box level. This technique combines dimensionality reduction and clustering methods that accurately separate the operational domains while also recognising outliers using well-fitting cluster borders. Various interpretability methods will be used for highlighting the cohesive factors amongst the operating regions and guide the understanding of the functionality as well. It will be shown that the introduction of a certain level of automation in the modelling part leads to a better understanding of the analysed data.

## Authorship and Acknowledgement
This document was written by me (Balázs Hain) under the supervison of [Prof. Dr. András Pataricza](https://scholar.google.hu/citations?user=7T1u-FgAAAAJ&hl=en) as a researcher, part of the Department of Measurement and Information Systems at BME, Hungary.